<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">

<html>
<head>
  <title></title>
  <meta http-equiv="content-type" content="text/html; charset=None">
  <style type="text/css">
td.linenos { background-color: #f0f0f0; padding-right: 10px; }
span.lineno { background-color: #f0f0f0; padding: 0 5px 0 5px; }
pre { line-height: 125%; }
body .hll { background-color: #ffffcc }
body  { background: #f8f8f8; }
body .c { color: #408080; font-style: italic } /* Comment */
body .err { border: 1px solid #FF0000 } /* Error */
body .k { color: #008000; font-weight: bold } /* Keyword */
body .o { color: #666666 } /* Operator */
body .cm { color: #408080; font-style: italic } /* Comment.Multiline */
body .cp { color: #BC7A00 } /* Comment.Preproc */
body .c1 { color: #408080; font-style: italic } /* Comment.Single */
body .cs { color: #408080; font-style: italic } /* Comment.Special */
body .gd { color: #A00000 } /* Generic.Deleted */
body .ge { font-style: italic } /* Generic.Emph */
body .gr { color: #FF0000 } /* Generic.Error */
body .gh { color: #000080; font-weight: bold } /* Generic.Heading */
body .gi { color: #00A000 } /* Generic.Inserted */
body .go { color: #888888 } /* Generic.Output */
body .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
body .gs { font-weight: bold } /* Generic.Strong */
body .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
body .gt { color: #0044DD } /* Generic.Traceback */
body .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
body .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
body .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
body .kp { color: #008000 } /* Keyword.Pseudo */
body .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
body .kt { color: #B00040 } /* Keyword.Type */
body .m { color: #666666 } /* Literal.Number */
body .s { color: #BA2121 } /* Literal.String */
body .na { color: #7D9029 } /* Name.Attribute */
body .nb { color: #008000 } /* Name.Builtin */
body .nc { color: #0000FF; font-weight: bold } /* Name.Class */
body .no { color: #880000 } /* Name.Constant */
body .nd { color: #AA22FF } /* Name.Decorator */
body .ni { color: #999999; font-weight: bold } /* Name.Entity */
body .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
body .nf { color: #0000FF } /* Name.Function */
body .nl { color: #A0A000 } /* Name.Label */
body .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
body .nt { color: #008000; font-weight: bold } /* Name.Tag */
body .nv { color: #19177C } /* Name.Variable */
body .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
body .w { color: #bbbbbb } /* Text.Whitespace */
body .mf { color: #666666 } /* Literal.Number.Float */
body .mh { color: #666666 } /* Literal.Number.Hex */
body .mi { color: #666666 } /* Literal.Number.Integer */
body .mo { color: #666666 } /* Literal.Number.Oct */
body .sb { color: #BA2121 } /* Literal.String.Backtick */
body .sc { color: #BA2121 } /* Literal.String.Char */
body .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
body .s2 { color: #BA2121 } /* Literal.String.Double */
body .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
body .sh { color: #BA2121 } /* Literal.String.Heredoc */
body .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
body .sx { color: #008000 } /* Literal.String.Other */
body .sr { color: #BB6688 } /* Literal.String.Regex */
body .s1 { color: #BA2121 } /* Literal.String.Single */
body .ss { color: #19177C } /* Literal.String.Symbol */
body .bp { color: #008000 } /* Name.Builtin.Pseudo */
body .vc { color: #19177C } /* Name.Variable.Class */
body .vg { color: #19177C } /* Name.Variable.Global */
body .vi { color: #19177C } /* Name.Variable.Instance */
body .il { color: #666666 } /* Literal.Number.Integer.Long */

  </style>
</head>
<body>
<h2></h2>

<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>

<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot; </span>
<span style="color: #BA2121; font-style: italic">This code was originally written for CS 231n at Stanford University</span>
<span style="color: #BA2121; font-style: italic">(cs231n.stanford.edu).  It has been modified in various areas for use in the</span>
<span style="color: #BA2121; font-style: italic">ECE 239AS class at UCLA.  This includes the descriptions of what code to</span>
<span style="color: #BA2121; font-style: italic">implement as well as some slight potential changes in variable names to be</span>
<span style="color: #BA2121; font-style: italic">consistent with class nomenclature.  We thank Justin Johnson &amp; Serena Yeung for</span>
<span style="color: #BA2121; font-style: italic">permission to use this code.  To see the original version, please visit</span>
<span style="color: #BA2121; font-style: italic">cs231n.stanford.edu.  </span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">TwoLayerNet</span>(<span style="color: #008000">object</span>):
	<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">	A two-layer fully-connected neural network. The net has an input dimension of</span>
<span style="color: #BA2121; font-style: italic">	N, a hidden layer dimension of H, and performs classification over C classes.</span>
<span style="color: #BA2121; font-style: italic">	We train the network with a softmax loss function and L2 regularization on the</span>
<span style="color: #BA2121; font-style: italic">	weight matrices. The network uses a ReLU nonlinearity after the first fully</span>
<span style="color: #BA2121; font-style: italic">	connected layer.</span>

<span style="color: #BA2121; font-style: italic">	In other words, the network has the following architecture:</span>

<span style="color: #BA2121; font-style: italic">	input - fully connected layer - ReLU - fully connected layer - softmax</span>

<span style="color: #BA2121; font-style: italic">	The outputs of the second fully-connected layer are the scores for each class.</span>
<span style="color: #BA2121; font-style: italic">	&quot;&quot;&quot;</span>

	<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, input_size, hidden_size, output_size, std<span style="color: #666666">=1e-4</span>):
		<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">		Initialize the model. Weights are initialized to small random values and</span>
<span style="color: #BA2121; font-style: italic">		biases are initialized to zero. Weights and biases are stored in the</span>
<span style="color: #BA2121; font-style: italic">		variable self.params, which is a dictionary with the following keys:</span>

<span style="color: #BA2121; font-style: italic">		W1: First layer weights; has shape (H, D)</span>
<span style="color: #BA2121; font-style: italic">		b1: First layer biases; has shape (H,)</span>
<span style="color: #BA2121; font-style: italic">		W2: Second layer weights; has shape (C, H)</span>
<span style="color: #BA2121; font-style: italic">		b2: Second layer biases; has shape (C,)</span>

<span style="color: #BA2121; font-style: italic">		Inputs:</span>
<span style="color: #BA2121; font-style: italic">		- input_size: The dimension D of the input data.</span>
<span style="color: #BA2121; font-style: italic">		- hidden_size: The number of neurons H in the hidden layer.</span>
<span style="color: #BA2121; font-style: italic">		- output_size: The number of classes C.</span>
<span style="color: #BA2121; font-style: italic">		&quot;&quot;&quot;</span>
		<span style="color: #008000">self</span><span style="color: #666666">.</span>params <span style="color: #666666">=</span> {}
		<span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;W1&#39;</span>] <span style="color: #666666">=</span> std <span style="color: #666666">*</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(hidden_size, input_size)
		<span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;b1&#39;</span>] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(hidden_size)
		<span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;W2&#39;</span>] <span style="color: #666666">=</span> std <span style="color: #666666">*</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(output_size, hidden_size)
		<span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;b2&#39;</span>] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(output_size)


	<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">loss</span>(<span style="color: #008000">self</span>, X, y<span style="color: #666666">=</span><span style="color: #008000">None</span>, reg<span style="color: #666666">=0.0</span>):
		<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">		Compute the loss and gradients for a two layer fully connected neural</span>
<span style="color: #BA2121; font-style: italic">		network.</span>

<span style="color: #BA2121; font-style: italic">		Inputs:</span>
<span style="color: #BA2121; font-style: italic">		- X: Input data of shape (N, D). Each X[i] is a training sample.</span>
<span style="color: #BA2121; font-style: italic">		- y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span>
<span style="color: #BA2121; font-style: italic">			an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span>
<span style="color: #BA2121; font-style: italic">			is not ed then we only return scores, and if it is ed then we</span>
<span style="color: #BA2121; font-style: italic">			instead return the loss and gradients.</span>
<span style="color: #BA2121; font-style: italic">		- reg: Regularization strength.</span>

<span style="color: #BA2121; font-style: italic">		Returns:</span>
<span style="color: #BA2121; font-style: italic">		If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span>
<span style="color: #BA2121; font-style: italic">		the score for class c on input X[i].</span>

<span style="color: #BA2121; font-style: italic">		If y is not None, instead return a tuple of:</span>
<span style="color: #BA2121; font-style: italic">		- loss: Loss (data loss and regularization loss) for this batch of training</span>
<span style="color: #BA2121; font-style: italic">			samples.</span>
<span style="color: #BA2121; font-style: italic">		- grads: Dictionary mapping parameter names to gradients of those parameters</span>
<span style="color: #BA2121; font-style: italic">			with respect to the loss function; has the same keys as self.params.</span>
<span style="color: #BA2121; font-style: italic">		&quot;&quot;&quot;</span>
		<span style="color: #408080; font-style: italic"># Unpack variables from the params dictionary</span>
		W1, b1 <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;W1&#39;</span>], <span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;b1&#39;</span>]
		W2, b2 <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;W2&#39;</span>], <span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;b2&#39;</span>]
		N, D <span style="color: #666666">=</span> X<span style="color: #666666">.</span>shape

		<span style="color: #408080; font-style: italic"># Compute the forward </span>
		scores <span style="color: #666666">=</span> <span style="color: #008000">None</span>

		<span style="color: #408080; font-style: italic"># ================================================================ #</span>
		<span style="color: #408080; font-style: italic"># YOUR CODE HERE:</span>
		<span style="color: #408080; font-style: italic">#   Calculate the output scores of the neural network.  The result</span>
		<span style="color: #408080; font-style: italic">#   should be (N, C). As stated in the description for this class,</span>
		<span style="color: #408080; font-style: italic">#	there should not be a ReLU layer after the second FC layer.</span>
		<span style="color: #408080; font-style: italic">#	The output of the second FC layer is the output scores. Do not</span>
		<span style="color: #408080; font-style: italic">#	use a for loop in your implementation.</span>
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>

		relu <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">lambda</span> x: x <span style="color: #666666">*</span> (x <span style="color: #666666">&gt;</span> <span style="color: #666666">0</span>)
		h1 <span style="color: #666666">=</span> relu(np<span style="color: #666666">.</span>dot(X, W1<span style="color: #666666">.</span>T) <span style="color: #666666">+</span> b1)
		out <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(h1, W2<span style="color: #666666">.</span>T) <span style="color: #666666">+</span> b2
		scores <span style="color: #666666">=</span> np<span style="color: #666666">.</span>copy(out) 
		
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>
		<span style="color: #408080; font-style: italic"># END YOUR CODE HERE</span>
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>

	
		<span style="color: #408080; font-style: italic"># If the targets are not given then jump out, we&#39;re done</span>
		<span style="color: #008000; font-weight: bold">if</span> y <span style="color: #AA22FF; font-weight: bold">is</span> <span style="color: #008000">None</span>:
			<span style="color: #008000; font-weight: bold">return</span> scores

		<span style="color: #408080; font-style: italic"># Compute the loss</span>
		loss <span style="color: #666666">=</span> <span style="color: #008000">None</span>

		<span style="color: #408080; font-style: italic"># ================================================================ #</span>
		<span style="color: #408080; font-style: italic"># YOUR CODE HERE:</span>
		<span style="color: #408080; font-style: italic">#   Calculate the loss of the neural network.  This includes the </span>
		<span style="color: #408080; font-style: italic"># 	softmax loss and the L2 regularization for W1 and W2. Store the </span>
		<span style="color: #408080; font-style: italic">#	total loss in teh variable loss.  Multiply the regularization</span>
		<span style="color: #408080; font-style: italic"># 	loss by 0.5 (in addition to the factor reg).</span>
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>

		<span style="color: #408080; font-style: italic"># scores is num_examples by num_classes</span>
		p <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(scores <span style="color: #666666">-</span> np<span style="color: #666666">.</span>max(scores, axis <span style="color: #666666">=</span> <span style="color: #666666">1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000">True</span>))
		p <span style="color: #666666">/=</span> np<span style="color: #666666">.</span>sum(p, axis<span style="color: #666666">=1</span>, keepdims <span style="color: #666666">=</span> <span style="color: #008000">True</span>)

		loss <span style="color: #666666">=</span> <span style="color: #666666">-</span>np<span style="color: #666666">.</span>sum(np<span style="color: #666666">.</span>log(p[np<span style="color: #666666">.</span>arange(N), y])) <span style="color: #666666">/</span> N

		ds <span style="color: #666666">=</span> p<span style="color: #666666">.</span>copy()
		ds[np<span style="color: #666666">.</span>arange(N), y] <span style="color: #666666">-=</span> <span style="color: #666666">1</span>
		ds <span style="color: #666666">/=</span> N

		dreg <span style="color: #666666">=</span> reg<span style="color: #666666">*0.5*</span>(np<span style="color: #666666">.</span>sum(W1<span style="color: #666666">**2</span>) <span style="color: #666666">+</span> np<span style="color: #666666">.</span>sum(W2<span style="color: #666666">**2</span>))

		loss <span style="color: #666666">+=</span> dreg
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>
		<span style="color: #408080; font-style: italic"># END YOUR CODE HERE</span>
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>

		grads <span style="color: #666666">=</span> {}

		<span style="color: #408080; font-style: italic"># ================================================================ #</span>
		<span style="color: #408080; font-style: italic"># YOUR CODE HERE:</span>
		<span style="color: #408080; font-style: italic"># 	Implement the backward .  Compute the derivatives of the </span>
		<span style="color: #408080; font-style: italic"># 	weights and the biases.  Store the results in the grads</span>
		<span style="color: #408080; font-style: italic">#	dictionary.  e.g., grads[&#39;W1&#39;] should store the gradient for </span>
		<span style="color: #408080; font-style: italic"># 	W1, and be of the same size as W1.</span>
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>

		grads[<span style="color: #BA2121">&#39;W2&#39;</span>] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(ds<span style="color: #666666">.</span>T ,h1) <span style="color: #666666">+</span> reg <span style="color: #666666">*</span> W2
		grads[<span style="color: #BA2121">&#39;b2&#39;</span>] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(np<span style="color: #666666">.</span>ones(N), ds)

		dh1 <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(ds, W2)
		dh1[h1<span style="color: #666666">==0</span>] <span style="color: #666666">=</span> <span style="color: #666666">0</span>

		grads[<span style="color: #BA2121">&#39;W1&#39;</span>] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(dh1<span style="color: #666666">.</span>T, X) <span style="color: #666666">+</span> reg<span style="color: #666666">*</span>W1
		grads[<span style="color: #BA2121">&#39;b1&#39;</span>] <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(np<span style="color: #666666">.</span>ones(N), dh1)

		<span style="color: #408080; font-style: italic"># ================================================================ #</span>
		<span style="color: #408080; font-style: italic"># END YOUR CODE HERE</span>
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>

		<span style="color: #008000; font-weight: bold">return</span> loss, grads

	<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">train</span>(<span style="color: #008000">self</span>, X, y, X_val, y_val,
						learning_rate<span style="color: #666666">=1e-3</span>, learning_rate_decay<span style="color: #666666">=0.95</span>,
						reg<span style="color: #666666">=1e-5</span>, num_iters<span style="color: #666666">=100</span>,
						batch_size<span style="color: #666666">=200</span>, verbose<span style="color: #666666">=</span><span style="color: #008000">False</span>):
		<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">		Train this neural network using stochastic gradient descent.</span>

<span style="color: #BA2121; font-style: italic">		Inputs:</span>
<span style="color: #BA2121; font-style: italic">		- X: A numpy array of shape (N, D) giving training data.</span>
<span style="color: #BA2121; font-style: italic">		- y: A numpy array f shape (N,) giving training labels; y[i] = c means that</span>
<span style="color: #BA2121; font-style: italic">			X[i] has label c, where 0 &lt;= c &lt; C.</span>
<span style="color: #BA2121; font-style: italic">		- X_val: A numpy array of shape (N_val, D) giving validation data.</span>
<span style="color: #BA2121; font-style: italic">		- y_val: A numpy array of shape (N_val,) giving validation labels.</span>
<span style="color: #BA2121; font-style: italic">		- learning_rate: Scalar giving learning rate for optimization.</span>
<span style="color: #BA2121; font-style: italic">		- learning_rate_decay: Scalar giving factor used to decay the learning rate</span>
<span style="color: #BA2121; font-style: italic">			after each epoch.</span>
<span style="color: #BA2121; font-style: italic">		- reg: Scalar giving regularization strength.</span>
<span style="color: #BA2121; font-style: italic">		- num_iters: Number of steps to take when optimizing.</span>
<span style="color: #BA2121; font-style: italic">		- batch_size: Number of training examples to use per step.</span>
<span style="color: #BA2121; font-style: italic">		- verbose: boolean; if true print progress during optimization.</span>
<span style="color: #BA2121; font-style: italic">		&quot;&quot;&quot;</span>
		num_train <span style="color: #666666">=</span> X<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>]
		iterations_per_epoch <span style="color: #666666">=</span> <span style="color: #008000">max</span>(num_train <span style="color: #666666">/</span> batch_size, <span style="color: #666666">1</span>)

		<span style="color: #408080; font-style: italic"># Use SGD to optimize the parameters in self.model</span>
		loss_history <span style="color: #666666">=</span> []
		train_acc_history <span style="color: #666666">=</span> []
		val_acc_history <span style="color: #666666">=</span> []

		<span style="color: #008000; font-weight: bold">for</span> it <span style="color: #AA22FF; font-weight: bold">in</span> np<span style="color: #666666">.</span>arange(num_iters):
			X_batch <span style="color: #666666">=</span> <span style="color: #008000">None</span>
			y_batch <span style="color: #666666">=</span> <span style="color: #008000">None</span>

			<span style="color: #408080; font-style: italic"># ================================================================ #</span>
			<span style="color: #408080; font-style: italic"># YOUR CODE HERE:</span>
			<span style="color: #408080; font-style: italic"># 	Create a minibatch by sampling batch_size samples randomly.</span>
			<span style="color: #408080; font-style: italic"># ================================================================ #</span>
			batch_indexes <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>choice(<span style="color: #008000">list</span>(<span style="color: #008000">range</span>(<span style="color: #008000">len</span>(X))), size<span style="color: #666666">=</span>batch_size, replace<span style="color: #666666">=</span><span style="color: #008000">True</span>)

			<span style="color: #408080; font-style: italic">#print(batch_indexes)</span>

			X_batch <span style="color: #666666">=</span> [X[i] <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> batch_indexes]
			y_batch <span style="color: #666666">=</span> [y[i] <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> batch_indexes]

			X_batch <span style="color: #666666">=</span> np<span style="color: #666666">.</span>vstack(X_batch)

			<span style="color: #408080; font-style: italic">#print(X_batch)</span>

			<span style="color: #408080; font-style: italic"># ================================================================ #</span>
			<span style="color: #408080; font-style: italic"># END YOUR CODE HERE</span>
			<span style="color: #408080; font-style: italic"># ================================================================ #</span>

			 <span style="color: #408080; font-style: italic"># Compute loss and gradients using the current minibatch</span>
			loss, grads <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>loss(X_batch, y<span style="color: #666666">=</span>y_batch, reg<span style="color: #666666">=</span>reg)
			loss_history<span style="color: #666666">.</span>append(loss)

			<span style="color: #408080; font-style: italic"># ================================================================ #</span>
			<span style="color: #408080; font-style: italic"># YOUR CODE HERE:</span>
			<span style="color: #408080; font-style: italic"># 	Perform a gradient descent step using the minibatch to update</span>
		<span style="color: #408080; font-style: italic"># 	all parameters (i.e., W1, W2, b1, and b2).</span>
			<span style="color: #408080; font-style: italic"># ================================================================ #</span>

			reg_fact <span style="color: #666666">=</span> <span style="color: #666666">1</span> <span style="color: #666666">-</span> learning_rate <span style="color: #666666">*</span> reg
			<span style="color: #408080; font-style: italic">#for key in self.params:</span>
			<span style="color: #408080; font-style: italic">#	self.params[key] = reg_fact * (self.params[key]) - learning_rate * grads[key]</span>

			<span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;W1&#39;</span>] <span style="color: #666666">=</span> reg_fact <span style="color: #666666">*</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;W1&#39;</span>] <span style="color: #666666">-</span> learning_rate <span style="color: #666666">*</span> grads[<span style="color: #BA2121">&#39;W1&#39;</span>]
			<span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;W2&#39;</span>] <span style="color: #666666">=</span> reg_fact <span style="color: #666666">*</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;W2&#39;</span>] <span style="color: #666666">-</span> learning_rate <span style="color: #666666">*</span> grads[<span style="color: #BA2121">&#39;W2&#39;</span>]
			<span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;b1&#39;</span>] <span style="color: #666666">=</span> reg_fact <span style="color: #666666">*</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;b1&#39;</span>] <span style="color: #666666">-</span> learning_rate <span style="color: #666666">*</span> grads[<span style="color: #BA2121">&#39;b1&#39;</span>]
			<span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;b2&#39;</span>] <span style="color: #666666">=</span> reg_fact <span style="color: #666666">*</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>params[<span style="color: #BA2121">&#39;b2&#39;</span>] <span style="color: #666666">-</span> learning_rate <span style="color: #666666">*</span> grads[<span style="color: #BA2121">&#39;b2&#39;</span>]

			<span style="color: #408080; font-style: italic"># ================================================================ #</span>
			<span style="color: #408080; font-style: italic"># END YOUR CODE HERE</span>
			<span style="color: #408080; font-style: italic"># ================================================================ #</span>

			<span style="color: #008000; font-weight: bold">if</span> verbose <span style="color: #AA22FF; font-weight: bold">and</span> it <span style="color: #666666">%</span> <span style="color: #666666">100</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
				<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&#39;iteration {} / {}: loss {}&#39;</span><span style="color: #666666">.</span>format(it, num_iters, loss))

			<span style="color: #408080; font-style: italic"># Every epoch, check train and val accuracy and decay learning rate.</span>
			<span style="color: #008000; font-weight: bold">if</span> it <span style="color: #666666">%</span> iterations_per_epoch <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
				<span style="color: #408080; font-style: italic"># Check accuracy</span>
				train_acc <span style="color: #666666">=</span> (<span style="color: #008000">self</span><span style="color: #666666">.</span>predict(X_batch) <span style="color: #666666">==</span> y_batch)<span style="color: #666666">.</span>mean()
				val_acc <span style="color: #666666">=</span> (<span style="color: #008000">self</span><span style="color: #666666">.</span>predict(X_val) <span style="color: #666666">==</span> y_val)<span style="color: #666666">.</span>mean()
				train_acc_history<span style="color: #666666">.</span>append(train_acc)
				val_acc_history<span style="color: #666666">.</span>append(val_acc)

				<span style="color: #408080; font-style: italic"># Decay learning rate</span>
				learning_rate <span style="color: #666666">*=</span> learning_rate_decay

		<span style="color: #008000; font-weight: bold">return</span> {
			<span style="color: #BA2121">&#39;loss_history&#39;</span>: loss_history,
			<span style="color: #BA2121">&#39;train_acc_history&#39;</span>: train_acc_history,
			<span style="color: #BA2121">&#39;val_acc_history&#39;</span>: val_acc_history,
		}

	<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">predict</span>(<span style="color: #008000">self</span>, X):
		<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">		Use the trained weights of this two-layer network to predict labels for</span>
<span style="color: #BA2121; font-style: italic">		data points. For each data point we predict scores for each of the C</span>
<span style="color: #BA2121; font-style: italic">		classes, and assign each data point to the class with the highest score.</span>

<span style="color: #BA2121; font-style: italic">		Inputs:</span>
<span style="color: #BA2121; font-style: italic">		- X: A numpy array of shape (N, D) giving N D-dimensional data points to</span>
<span style="color: #BA2121; font-style: italic">			classify.</span>

<span style="color: #BA2121; font-style: italic">		Returns:</span>
<span style="color: #BA2121; font-style: italic">		- y_pred: A numpy array of shape (N,) giving predicted labels for each of</span>
<span style="color: #BA2121; font-style: italic">			the elements of X. For all i, y_pred[i] = c means that X[i] is predicted</span>
<span style="color: #BA2121; font-style: italic">			to have class c, where 0 &lt;= c &lt; C.</span>
<span style="color: #BA2121; font-style: italic">		&quot;&quot;&quot;</span>
		y_pred <span style="color: #666666">=</span> <span style="color: #008000">None</span>

		<span style="color: #408080; font-style: italic"># ================================================================ #</span>
		<span style="color: #408080; font-style: italic"># YOUR CODE HERE:</span>
		<span style="color: #408080; font-style: italic"># 	Predict the class given the input data.</span>
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>
		
		y_pred <span style="color: #666666">=</span> np<span style="color: #666666">.</span>argmax(<span style="color: #008000">self</span><span style="color: #666666">.</span>loss(X), axis<span style="color: #666666">=1</span>)



		<span style="color: #408080; font-style: italic"># ================================================================ #</span>
		<span style="color: #408080; font-style: italic"># END YOUR CODE HERE</span>
		<span style="color: #408080; font-style: italic"># ================================================================ #</span>

		<span style="color: #008000; font-weight: bold">return</span> y_pred
</pre></div>
</body>
</html>
