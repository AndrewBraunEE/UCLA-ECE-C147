{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##4QAM, 16QAM, 64QAM, BPSK, 8PSK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, regularizers\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitions to automatically generate symbols of QAM and M-Ary PSK\n",
    "def gray_code(n):\n",
    "    if n < 1:\n",
    "        g = []\n",
    "    else:\n",
    "        g = ['0', '1']\n",
    "        n -= 1\n",
    "        while n > 0:\n",
    "            k = len(g)\n",
    "            for i in range(k-1, -1, -1):\n",
    "                char = '1' + g[i]\n",
    "                g.append(char)\n",
    "            for i in range(k-1, -1, -1):\n",
    "                g[i] = '0' + g[i]\n",
    "            n -= 1\n",
    "    return g\n",
    "\n",
    "class BasicModem:\n",
    "    def TX(self, payload_data): #String type input\n",
    "        syms_out = np.zeros(int(len(payload_data)/self.bit_num), dtype=np.complex64)\n",
    "        cnt = 0\n",
    "        for i in range(0, len(payload_data) - self.bit_num, self.bit_num):\n",
    "            syms_out[cnt] = self.inv_dict[payload_data[i:i+self.bit_num]]\n",
    "            cnt += 1\n",
    "        return syms_out\n",
    "\n",
    "class QAM(BasicModem):\n",
    "    def __init__(self, NUM_SYMBOLS = 4, SAMP_RATE = 1):\n",
    "        self.NUM_SYMBOLS = NUM_SYMBOLS\n",
    "        self.SAMP_RATE = SAMP_RATE\n",
    "        dim_iq_axis = int(np.sqrt(NUM_SYMBOLS))\n",
    "        self.coords = []\n",
    "        self.symbols = gray_code(np.log2(NUM_SYMBOLS))\n",
    "        MAX_ENERGY = np.sqrt(2) #For QAM with coordinates at (x=1,y=1), normalized energy: divide by sqrt(2)\n",
    "        x = np.linspace(-1, 1, dim_iq_axis, dtype='complex64')\n",
    "        x, y = np.meshgrid(x, x)\n",
    "        self.coords = (x + y * 1j).flatten()\n",
    "        self.dict = dict(zip(self.coords, self.symbols))\n",
    "        self.inv_dict = dict(zip(self.symbols, self.coords))\n",
    "        self.bit_num = int(np.log2(NUM_SYMBOLS))\n",
    "\n",
    "class PSK(BasicModem):\n",
    "    def __init__(self, NUM_SYMBOLS = 8, SAMP_RATE = 1):\n",
    "        self.NUM_SYMBOLS = NUM_SYMBOLS\n",
    "        self.SAMP_RATE = SAMP_RATE\n",
    "        self.coords = np.zeros(NUM_SYMBOLS, dtype='complex64')\n",
    "        self.symbols = gray_code(np.log2(NUM_SYMBOLS))\n",
    "        cnt = 0\n",
    "        for i in np.arange(0, 2*np.pi, 2*np.pi/NUM_SYMBOLS):\n",
    "            #i is the phase\n",
    "            self.coords[cnt] =  1*np.exp(1j*i)\n",
    "            cnt += 1\n",
    "        #Above generates an array with the approximate locations of the symbols.\n",
    "        self.dict = dict(zip(self.coords, self.symbols))\n",
    "        self.inv_dict = dict(zip(self.symbols, self.coords))\n",
    "        self.bit_num = int(np.log2(NUM_SYMBOLS))\n",
    "        print(self.dict)\n",
    "\n",
    "class rayleigh_multipath:\n",
    "    \"\"\" a multipath channel with Rayleigh fading and AWGN \"\"\"\n",
    "    def __init__(self, var_awgn, var_rayleigh, pdp):\n",
    "        self.sigma_awgn = np.sqrt(var_awgn)\n",
    "        self.sigma_rayleigh = np.sqrt(var_rayleigh)\n",
    "        self.pdp = np.array(pdp)\n",
    "        self.l = self.pdp.size-1\n",
    "        self.update_cir()\n",
    "\n",
    "    def update_cir(self):\n",
    "        \"\"\" generate a new CIR from the PDP with Rayleigh fading \"\"\"\n",
    "        self.cir = np.sqrt(np.array(self.pdp))\n",
    "        randray = np.random.rayleigh(self.sigma_rayleigh, self.cir.size)\n",
    "        self.cir = self.cir*randray\n",
    "\n",
    "    def awgn(self, symbols):\n",
    "        \"\"\" add white Gaussian noise \"\"\"\n",
    "        real_noise = np.random.randn(symbols.size)\n",
    "        imag_noise = np.random.randn(symbols.size)\n",
    "        noise = real_noise+1j*imag_noise\n",
    "        return symbols+self.sigma_awgn*noise\n",
    "\n",
    "    def apply_cir(self, symbols):\n",
    "        \"\"\" convolve the symbols with the CIR \"\"\"\n",
    "        if self.l != 0:\n",
    "            self.old_symbols = symbols[-self.l:]\n",
    "        # apply the cir\n",
    "        symbols = np.convolve(symbols, self.cir)\n",
    "        return symbols \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1+0j): '0', (-1+1.2246469e-16j): '1'}\n",
      "{(1+0j): '0000', (0.9238795+0.38268343j): '0001', (0.70710677+0.70710677j): '0011', (0.38268343+0.9238795j): '0010', (6.123234e-17+1j): '0110', (-0.38268343+0.9238795j): '0111', (-0.70710677+0.70710677j): '0101', (-0.9238795+0.38268343j): '0100', (-1+1.2246469e-16j): '1100', (-0.9238795-0.38268343j): '1101', (-0.70710677-0.70710677j): '1111', (-0.38268343-0.9238795j): '1110', (-1.8369701e-16-1j): '1010', (0.38268343-0.9238795j): '1011', (0.70710677-0.70710677j): '1001', (0.9238795-0.38268343j): '1000'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.+1.j, 1.-1.j, 0.+0.j], dtype=complex64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myqam4 = QAM(NUM_SYMBOLS=4)\n",
    "myqam16 = QAM(NUM_SYMBOLS=16)\n",
    "myqam64 = QAM(NUM_SYMBOLS=64)\n",
    "myBPSK = PSK(NUM_SYMBOLS = 2)\n",
    "myPSK8 = PSK(NUM_SYMBOLS = 16)\n",
    "myqam4.TX('100100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: CommPy contributors\n",
    "# License: BSD 3-Clause\n",
    "\n",
    "\"\"\"\n",
    "==================================================\n",
    "Modulation Demodulation (:mod:`commpy.modulation`)\n",
    "==================================================\n",
    ".. autosummary::\n",
    "   :toctree: generated/\n",
    "   PSKModem             -- Phase Shift Keying (PSK) Modem.\n",
    "   QAMModem             -- Quadrature Amplitude Modulation (QAM) Modem.\n",
    "   ofdm_tx              -- OFDM Transmit Signal Generation\n",
    "   ofdm_rx              -- OFDM Receive Signal Processing\n",
    "   mimo_ml              -- MIMO Maximum Likelihood (ML) Detection.\n",
    "   kbest                -- MIMO K-best Schnorr-Euchner Detection.\n",
    "   bit_lvl_repr         -- Bit level representation.\n",
    "   max_log_approx       -- Max-log approximation.\n",
    "\"\"\"\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange, array, zeros, pi, cos, sin, sqrt, log2, argmin, \\\n",
    "    hstack, repeat, tile, dot, shape, concatenate, exp, \\\n",
    "    log, vectorize, empty, eye, kron, inf\n",
    "from numpy.fft import fft, ifft\n",
    "from numpy.linalg import qr, norm\n",
    "\n",
    "from commpy.utilities import bitarray2dec, dec2bitarray\n",
    "\n",
    "__all__ = ['PSKModem', 'QAMModem', 'ofdm_tx', 'ofdm_rx', 'mimo_ml', 'kbest', 'bit_lvl_repr', 'max_log_approx']\n",
    "\n",
    "\n",
    "class Modem:\n",
    "    def modulate(self, input_bits):\n",
    "        \"\"\" Modulate (map) an array of bits to constellation symbols.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_bits : 1D ndarray of ints\n",
    "            Inputs bits to be modulated (mapped).\n",
    "        Returns\n",
    "        -------\n",
    "        baseband_symbols : 1D ndarray of complex floats\n",
    "            Modulated complex symbols.\n",
    "        \"\"\"\n",
    "        mapfunc = vectorize(lambda i:\n",
    "                            self.constellation[bitarray2dec(input_bits[i:i + self.num_bits_symbol])])\n",
    "\n",
    "        baseband_symbols = mapfunc(arange(0, len(input_bits), self.num_bits_symbol))\n",
    "\n",
    "        return baseband_symbols\n",
    "\n",
    "    def demodulate(self, input_symbols, demod_type, noise_var=0):\n",
    "        \"\"\" Demodulate (map) a set of constellation symbols to corresponding bits.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_symbols : 1D ndarray of complex floats\n",
    "            Input symbols to be demodulated.\n",
    "        demod_type : string\n",
    "            'hard' for hard decision output (bits)\n",
    "            'soft' for soft decision output (LLRs)\n",
    "        noise_var : float\n",
    "            AWGN variance. Needs to be specified only if demod_type is 'soft'\n",
    "        Returns\n",
    "        -------\n",
    "        demod_bits : 1D ndarray of ints\n",
    "            Corresponding demodulated bits.\n",
    "        \"\"\"\n",
    "        if demod_type == 'hard':\n",
    "            index_list = map(lambda i: argmin(abs(input_symbols[i] - self.constellation)),\n",
    "                             range(0, len(input_symbols)))\n",
    "            demod_bits = array([dec2bitarray(i, self.num_bits_symbol) for i in index_list]).reshape(-1)\n",
    "\n",
    "        elif demod_type == 'soft':\n",
    "            demod_bits = zeros(len(input_symbols) * self.num_bits_symbol)\n",
    "            for i in arange(len(input_symbols)):\n",
    "                current_symbol = input_symbols[i]\n",
    "                for bit_index in arange(self.num_bits_symbol):\n",
    "                    llr_num = 0\n",
    "                    llr_den = 0\n",
    "                    for const_index in self.symbol_mapping:\n",
    "                        if (const_index >> bit_index) & 1:\n",
    "                            llr_num = llr_num + exp(\n",
    "                                (-abs(current_symbol - self.constellation[const_index]) ** 2) / noise_var)\n",
    "                        else:\n",
    "                            llr_den = llr_den + exp(\n",
    "                                (-abs(current_symbol - self.constellation[const_index]) ** 2) / noise_var)\n",
    "                    demod_bits[i * self.num_bits_symbol + self.num_bits_symbol - 1 - bit_index] = log(llr_num / llr_den)\n",
    "        else:\n",
    "            raise ValueError('demod_type must be \"hard\" or \"soft\"')\n",
    "\n",
    "        return demod_bits\n",
    "\n",
    "    def plot_constellation(self):\n",
    "        \"\"\" Plot the constellation \"\"\"\n",
    "        # init some arrays\n",
    "        beta = self.num_bits_symbol\n",
    "        numbit = '0' + str(beta) + 'b'\n",
    "        Bin = []\n",
    "        mot = []\n",
    "        const = []\n",
    "\n",
    "        # creation of w array\n",
    "        reel = [pow(2, i) for i in range(beta // 2 - 1, -1, -1)]\n",
    "        im = [1j * pow(2, i) for i in range(beta // 2 - 1, -1, -1)]\n",
    "        w = concatenate((reel, im), axis=None)\n",
    "\n",
    "        listBin = [format(i, numbit) for i in range(2 ** beta)]\n",
    "        for e in listBin:\n",
    "            for i in range(beta):\n",
    "                Bin.append(ord(e[i]) - 48)\n",
    "                if ord(e[i]) - 48 == 0:\n",
    "                    mot.append(-1)\n",
    "                else:\n",
    "                    mot.append(1)\n",
    "            const.append(dot(w, mot))\n",
    "            mot = []\n",
    "        symb = self.modulate(Bin)\n",
    "\n",
    "        # plot the symbols\n",
    "        x = symb.real\n",
    "        y = symb.imag\n",
    "\n",
    "        plt.plot(x, y, '+', linewidth=4)\n",
    "        for i in range(len(x)):\n",
    "            plt.text(x[i], y[i], listBin[i])\n",
    "\n",
    "        plt.title('Constellation')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class PSKModem(Modem):\n",
    "    \"\"\" Creates a Phase Shift Keying (PSK) Modem object. \"\"\"\n",
    "\n",
    "    Es = 1\n",
    "\n",
    "    def _constellation_symbol(self, i):\n",
    "        return cos(2 * pi * (i - 1) / self.m) + sin(2 * pi * (i - 1) / self.m) * (0 + 1j)\n",
    "\n",
    "    def __init__(self, m):\n",
    "        \"\"\" Creates a Phase Shift Keying (PSK) Modem object.\n",
    "        Parameters\n",
    "        ----------\n",
    "        m : int\n",
    "            Size of the PSK constellation.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.num_bits_symbol = int(log2(self.m))\n",
    "        self.symbol_mapping = arange(self.m)\n",
    "        self.constellation = list(map(self._constellation_symbol,\n",
    "                                      self.symbol_mapping))\n",
    "\n",
    "\n",
    "class QAMModem(Modem):\n",
    "    \"\"\" Creates a Quadrature Amplitude Modulation (QAM) Modem object.\"\"\"\n",
    "\n",
    "    def _constellation_symbol(self, i):\n",
    "        return (2 * i[0] - 1) + (2 * i[1] - 1) * (1j)\n",
    "\n",
    "    def __init__(self, m):\n",
    "        \"\"\" Creates a Quadrature Amplitude Modulation (QAM) Modem object.\n",
    "        Parameters\n",
    "        ----------\n",
    "        m : int\n",
    "            Size of the QAM constellation.\n",
    "        \"\"\"\n",
    "\n",
    "        self.m = m\n",
    "        self.num_bits_symbol = int(log2(self.m))\n",
    "        self.symbol_mapping = arange(self.m)\n",
    "        mapping_array = arange(1, sqrt(self.m) + 1) - (sqrt(self.m) / 2)\n",
    "        self.constellation = list(map(self._constellation_symbol,\n",
    "                                      list(product(mapping_array, repeat=2))))\n",
    "        self.Es = 2 * (self.m - 1) / 3\n",
    "\n",
    "\n",
    "def ofdm_tx(x, nfft, nsc, cp_length):\n",
    "    \"\"\" OFDM Transmit signal generation \"\"\"\n",
    "\n",
    "    nfft = float(nfft)\n",
    "    nsc = float(nsc)\n",
    "    cp_length = float(cp_length)\n",
    "    ofdm_tx_signal = array([])\n",
    "\n",
    "    for i in range(0, shape(x)[1]):\n",
    "        symbols = x[:, i]\n",
    "        ofdm_sym_freq = zeros(nfft, dtype=complex)\n",
    "        ofdm_sym_freq[1:(nsc / 2) + 1] = symbols[nsc / 2:]\n",
    "        ofdm_sym_freq[-(nsc / 2):] = symbols[0:nsc / 2]\n",
    "        ofdm_sym_time = ifft(ofdm_sym_freq)\n",
    "        cp = ofdm_sym_time[-cp_length:]\n",
    "        ofdm_tx_signal = concatenate((ofdm_tx_signal, cp, ofdm_sym_time))\n",
    "\n",
    "    return ofdm_tx_signal\n",
    "\n",
    "\n",
    "def ofdm_rx(y, nfft, nsc, cp_length):\n",
    "    \"\"\" OFDM Receive Signal Processing \"\"\"\n",
    "\n",
    "    num_ofdm_symbols = int(len(y) / (nfft + cp_length))\n",
    "    x_hat = zeros([nsc, num_ofdm_symbols], dtype=complex)\n",
    "\n",
    "    for i in range(0, num_ofdm_symbols):\n",
    "        ofdm_symbol = y[i * nfft + (i + 1) * cp_length:(i + 1) * (nfft + cp_length)]\n",
    "        symbols_freq = fft(ofdm_symbol)\n",
    "        x_hat[:, i] = concatenate((symbols_freq[-nsc / 2:], symbols_freq[1:(nsc / 2) + 1]))\n",
    "\n",
    "    return x_hat\n",
    "\n",
    "\n",
    "def mimo_ml(y, h, constellation):\n",
    "    \"\"\" MIMO ML Detection.\n",
    "    parameters\n",
    "    ----------\n",
    "    y : 1D ndarray of complex floats\n",
    "        Received complex symbols (shape: num_receive_antennas x 1)\n",
    "    h : 2D ndarray of complex floats\n",
    "        Channel Matrix (shape: num_receive_antennas x num_transmit_antennas)\n",
    "    constellation : 1D ndarray of complex floats\n",
    "        Constellation used to modulate the symbols\n",
    "    \"\"\"\n",
    "    _, n = h.shape\n",
    "    m = len(constellation)\n",
    "    x_ideal = empty((n, pow(m, n)), complex)\n",
    "    for i in range(0, n):\n",
    "        x_ideal[i] = repeat(tile(constellation, pow(m, i)), pow(m, n - i - 1))\n",
    "    min_idx = argmin(norm(y[:, None] - dot(h, x_ideal), axis=0))\n",
    "    x_r = x_ideal[:, min_idx]\n",
    "\n",
    "    return x_r\n",
    "\n",
    "\n",
    "def kbest(y, h, constellation, K, noise_var=0, output_type='hard', demode=None):\n",
    "    \"\"\" MIMO K-best Schnorr-Euchner Detection.\n",
    "    Reference: Zhan Guo and P. Nilsson, 'Algorithm and implementation of the K-best sphere decoding for MIMO detection',\n",
    "        IEEE Journal on Selected Areas in Communications, vol. 24, no. 3, pp. 491-503, Mar. 2006.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : 1D ndarray\n",
    "        Received complex symbols (length: num_receive_antennas)\n",
    "    h : 2D ndarray\n",
    "        Channel Matrix (shape: num_receive_antennas x num_transmit_antennas)\n",
    "    constellation : 1D ndarray of floats\n",
    "        Constellation used to modulate the symbols\n",
    "    K : positive integer\n",
    "        Number of candidates kept at each step\n",
    "    noise_var : positive float\n",
    "        Noise variance.\n",
    "        *Default* value is 0.\n",
    "    output_type : str\n",
    "        'hard': hard output i.e. output is a binary word\n",
    "        'soft': soft output i.e. output is a vector of Log-Likelihood Ratios.\n",
    "        *Default* value is 'hard'\n",
    "    demode : function with prototype binary_word = demode(point)\n",
    "        Function that provide the binary word corresponding to a symbol vector.\n",
    "    Returns\n",
    "    -------\n",
    "    x : 1D ndarray of constellation points or of Log-Likelihood Ratios.\n",
    "        Detected vector (length: num_receive_antennas).\n",
    "    raises\n",
    "    ------\n",
    "    ValueError\n",
    "                If h has more columns than rows.\n",
    "                If output_type is something else than 'hard' or 'soft'\n",
    "    \"\"\"\n",
    "    nb_tx, nb_rx = h.shape\n",
    "    if nb_rx > nb_tx:\n",
    "        raise ValueError('h has more columns than rows')\n",
    "\n",
    "    # QR decomposition\n",
    "    q, r = qr(h)\n",
    "    yt = q.conj().T.dot(y)\n",
    "\n",
    "    # Initialization\n",
    "    m = len(constellation)\n",
    "    nb_can = 1\n",
    "\n",
    "    if isinstance(constellation[0], complex):\n",
    "        const_type = complex\n",
    "    else:\n",
    "        const_type = float\n",
    "    X = empty((nb_rx, K * m), dtype=const_type)  # Set of current candidates\n",
    "    d = tile(yt[:, None], (1, K * m))  # Corresponding distance vector\n",
    "    d_tot = zeros(K * m, dtype=float)  # Corresponding total distance\n",
    "    hyp = empty(K * m, dtype=const_type)  # Hypothesis vector\n",
    "\n",
    "    # Processing\n",
    "    for coor in range(nb_rx - 1, -1, -1):\n",
    "        nb_hyp = nb_can * m\n",
    "\n",
    "        # Copy best candidates m times\n",
    "        X[:, :nb_hyp] = tile(X[:, :nb_can], (1, m))\n",
    "        d[:, :nb_hyp] = tile(d[:, :nb_can], (1, m))\n",
    "        d_tot[:nb_hyp] = tile(d_tot[:nb_can], (1, m))\n",
    "\n",
    "        # Make hypothesis\n",
    "        hyp[:nb_hyp] = repeat(constellation, nb_can)\n",
    "        X[coor, :nb_hyp] = hyp[:nb_hyp]\n",
    "        d[coor, :nb_hyp] -= r[coor, coor] * hyp[:nb_hyp]\n",
    "        d_tot[:nb_hyp] += abs(d[coor, :nb_hyp]) ** 2\n",
    "\n",
    "        # Select best candidates\n",
    "        argsort = d_tot[:nb_hyp].argsort()\n",
    "        nb_can = min(nb_hyp, K)  # Update number of candidate\n",
    "\n",
    "        # Update accordingly\n",
    "        X[:, :nb_can] = X[:, argsort[:nb_can]]\n",
    "        d[:, :nb_can] = d[:, argsort[:nb_can]]\n",
    "        d[:coor, :nb_can] -= r[:coor, coor, None] * hyp[argsort[:nb_can]]\n",
    "        d_tot[:nb_can] = d_tot[argsort[:nb_can]]\n",
    "\n",
    "    if output_type == 'hard':\n",
    "        return X[:, 0]\n",
    "    elif output_type == 'soft':\n",
    "        return max_log_approx(y, h, noise_var, X[:, :nb_can], demode)\n",
    "    else:\n",
    "        raise ValueError('output_type must be \"hard\" or \"soft\"')\n",
    "\n",
    "\n",
    "def bit_lvl_repr(H, w):\n",
    "    \"\"\" Bit-level representation of matrix H with weights w.\n",
    "    parameters\n",
    "    ----------\n",
    "    H   :   2D ndarray (shape : nb_rx, nb_tx)\n",
    "            Channel Matrix.\n",
    "    w   :   1D ndarray of complex (length : beta)\n",
    "            Bit level representation weights. The length must be even.\n",
    "    return\n",
    "    ------\n",
    "    A : 2D nbarray (shape : nb_rx, nb_tx*beta)\n",
    "        Channel matrix adapted to the bit-level representation.\n",
    "    \"\"\"\n",
    "    beta = len(w)\n",
    "    if beta % 2 == 0:\n",
    "        m, n = H.shape\n",
    "        In = eye(n, n)\n",
    "        kr = kron(In, w)\n",
    "        return dot(H, kr)\n",
    "    else:\n",
    "        raise ValueError('Beta must be even.')\n",
    "\n",
    "\n",
    "def max_log_approx(y, h, noise_var, pts_list, demode):\n",
    "    \"\"\" Max-log demode\n",
    "    parameters\n",
    "    ----------\n",
    "    y : 1D ndarray\n",
    "        Received symbol vector (length: num_receive_antennas)\n",
    "    h : 2D ndarray\n",
    "        Channel Matrix (shape: num_receive_antennas x num_transmit_antennas)\n",
    "    noise_var : positive float\n",
    "        Noise variance\n",
    "    pts_list : 2D ndarray of constellation points\n",
    "        Set of points to compute max log approximation (points are column-wise).\n",
    "        (shape: num_receive_antennas x num_points)\n",
    "    demode : function with prototype binary_word = demode(point)\n",
    "        Function that provide the binary word corresponding to a symbol vector.\n",
    "    return\n",
    "    ------\n",
    "    LLR : 1D ndarray of floats\n",
    "        Log-Likelihood Ratio for each bit (same length as the return of decode)\n",
    "    \"\"\"\n",
    "    # Decode all pts\n",
    "    nb_pts = pts_list.shape[1]\n",
    "    bits = demode(pts_list.reshape(-1, order='F')).reshape(nb_pts, -1)  # Set of binary words (one word by row)\n",
    "\n",
    "    # Prepare LLR computation\n",
    "    nb_bits = bits.shape[1]\n",
    "    LLR = empty(nb_bits)\n",
    "\n",
    "    # Loop for each bit\n",
    "    for k in range(nb_bits):\n",
    "        # Select pts based on the k-th bit in the corresponding word\n",
    "        pts0 = pts_list.compress(bits[:, k] == 0, axis=1)\n",
    "        pts1 = pts_list.compress(bits[:, k] == 1, axis=1)\n",
    "\n",
    "        # Compute the norms and add inf to handle empty set of points\n",
    "        norms0 = hstack((norm(y[:, None] - h.dot(pts0), axis=0) ** 2, inf))\n",
    "        norms1 = hstack((norm(y[:, None] - h.dot(pts1), axis=0) ** 2, inf))\n",
    "\n",
    "        # Compute LLR\n",
    "        LLR[k] = min(norms0) - min(norms1)\n",
    "\n",
    "    return -LLR / (2 * noise_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Note that the QAM soft-demodulators simply dont work due to a divide by zero error!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qam4 = QAMModem(4)\n",
    "qam16 = QAMModem(16)\n",
    "qam64 = QAMModem(64)\n",
    "bpsk = PSKModem(2)\n",
    "psk8 = PSKModem(8)\n",
    "'''Note that the QAM soft-demodulators simply dont work due to a divide by zero error!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "def P_error_mqam(M, Es_No_Ratio, Es = None, No = None):\n",
    "    if Es != None and No != None:\n",
    "        return 2*(1-1/(np.sqrt(M)))*special.erfc(k*np.sqrt(Es/No))\n",
    "    else:\n",
    "        return 2*(1-1/(np.sqrt(M)))*special.erfc(k*np.sqrt(Es_No_Ratio))\n",
    "\n",
    "def P_error_mpsk(M, Es_No_Ratio, Es=None, No = None):\n",
    "    if Es != None and No != None:\n",
    "        return 2 * special.erfc(np.sqrt(2*(Es/No)*sin(np.pi/M)**2))\n",
    "    else:\n",
    "        return 2 * special.erfc(np.sqrt(2*(Es_No_Ratio)*sin(np.pi/M)**2))\n",
    "\n",
    "def P_error_bpsk(Es_No_Ratio, Es = None, No = None):\n",
    "    if Es != None and No != None:\n",
    "        return 0.5*special.erfc(np.sqrt(Es_No_Ratio))\n",
    "    else:\n",
    "        return 0.5*special.erfc(np.sqrt(Es/No))\n",
    "\n",
    "def calc_llr_wrong(symbol_mapping, r, s, h = np.complex64(1.0), No = 1, var = 1):\n",
    "    #https://arxiv.org/pdf/1903.04656.pdf\n",
    "    #I might be doing this wrong, this might supposed to be Probability(received symbol | bit of i = 1)\n",
    "    null_symbols = []\n",
    "    null_bits = []\n",
    "    my_symbol = []\n",
    "    my_bits = []\n",
    "    for key,value in symbol_mapping:\n",
    "        if s == key:\n",
    "            my_symbol.append(key)\n",
    "            my_bits.append(value)\n",
    "        else:\n",
    "            null_symbol.append(key)\n",
    "            null_symbol.append(value)\n",
    "            \n",
    "    a = np.exp(-1*(np.mag(r-h*my_symbol[0]))/var) #There is only one correct symbol\n",
    "    \n",
    "    running_sum = 0.0\n",
    "    for i in range(null_symbols):\n",
    "        b = np.exp(-1*(np.mag(r-h*null_symbols[i]))/var)\n",
    "        running_sum += b\n",
    "    \n",
    "def calc_llr_v0(symbol_mapping, r, s, h = np.complex64(1.0), No = 1, var = 1):\n",
    "    #https://arxiv.org/pdf/1903.04656.pdf\n",
    "    #I might be doing this wrong, this might supposed to be Probability(received symbol | bit of i = 1)\n",
    "    #s is the label, r is the received symbol\n",
    "    null_symbols = []\n",
    "    null_bits = []\n",
    "    my_symbol = []\n",
    "    my_bits = []\n",
    "    #print(symbol_mapping)\n",
    "    for key,value in symbol_mapping.items():\n",
    "        #print(\"for\", s, key)\n",
    "        if s == key:\n",
    "            my_symbol.append(key)\n",
    "            my_bits.append(value)\n",
    "        else:\n",
    "            null_symbols.append(key)\n",
    "            null_bits.append(value)\n",
    "    #print(my_bits)\n",
    "    run_sum_1 = 0.0\n",
    "    run_sum_0 = 0.0\n",
    "    for bit in my_bits[0]:\n",
    "        a = np.exp(-1*(np.linalg.norm(r-h*s))/var) #There is only one correct symbol\n",
    "        #print(a, bit)\n",
    "        if bit == '0':\n",
    "            run_sum_0 += a\n",
    "        else:\n",
    "            run_sum_1 += a\n",
    "    \n",
    "    Li = np.log2(run_sum_1) - np.log2(run_sum_0)\n",
    "    #If 0: Equally likely\n",
    "    #print(Li)\n",
    "    return Li\n",
    "\n",
    "def has_ith_bit(i = 0, bit = '0', symbol_mapping = {}):\n",
    "    sym_list = []\n",
    "    for symbol, bits in symbol_mapping.items():\n",
    "        if bits[i] == bit:\n",
    "            sym_list.append(symbol)\n",
    "    return sym_list\n",
    "\n",
    "def calc_llr(symbol_mapping, r, h = np.complex64(1.0), var = 1):\n",
    "    #https://arxiv.org/pdf/1903.04656.pdf\n",
    "    #I might be doing this wrong, this might supposed to be Probability(received symbol | bit of i = 1)\n",
    "    #s is the label, r is the received symbol\n",
    "    Ls = []\n",
    "    vals = list(symbol_mapping.values())\n",
    "    for i in range(len(vals[0])):\n",
    "        soft_symbols_1 = has_ith_bit(i = i, bit = '1', symbol_mapping = symbol_mapping)\n",
    "        soft_symbols_0 = has_ith_bit(i = i, bit = '0', symbol_mapping = symbol_mapping)\n",
    "        run_sum_1 = np.float64(0.0)\n",
    "        run_sum_0 = np.float64(0.0)\n",
    "        for local_s in soft_symbols_1:\n",
    "            a = np.exp(-1*(np.linalg.norm(r-h*local_s))/var) #Minimum is to fix growing exponential bug for each SNR\n",
    "            run_sum_1 += a\n",
    "        for local_s in soft_symbols_0:\n",
    "            a = np.exp(-1*(np.linalg.norm(r-h*local_s))/var) #Minimum is to fix growing exponential bug for each SNR\n",
    "            run_sum_0 += a\n",
    "        #print('runs1: ', run_sum_1, 'runs0: ', run_sum_0)\n",
    "        Li = np.float64(np.log2(run_sum_1)) - np.float64(np.log2(run_sum_0))\n",
    "        Ls.append(Li)\n",
    "    #print(Ls)\n",
    "    return Ls\n",
    "\n",
    "    \n",
    "def soft_demodulate(input_symbols, symbol_mapping, h = np.complex64(1.0), var = 1):\n",
    "    #Code taken from CommPy: https://github.com/veeresht/CommPy/blob/master/commpy/modulation.py\n",
    "    list_syms = np.array([ k for k in symbol_mapping.keys() ])\n",
    "    list_bits = np.array([ v for v in symbol_mapping.values() ])\n",
    "    str_out = ''\n",
    "    for i in range(len(input_symbols)):\n",
    "        llrs = calc_llr(symbol_mapping, input_symbols[i], h = h, var = var)\n",
    "        for llr in llrs:\n",
    "            if llr <= 0.0:\n",
    "                str_out += '0'\n",
    "            else:\n",
    "                str_out += '1'\n",
    "    return str_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateModel(in_shape = 2, bps = 2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(in_shape, input_shape=(in_shape, )))\n",
    "    model.add(Dense(in_shape*64, input_shape=(in_shape, )))\n",
    "    #leakyrelu = keras.layers.LeakyReLU(alpha=0.3)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(bps, input_shape=(bps, ), \n",
    "                   kernel_regularizer=regularizers.l2(0.0001)\n",
    "                    ,\n",
    "                    activity_regularizer=regularizers.l1(0.0001)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00\n",
      "0000\n",
      "calc_llr: [-2.5457138269082145, -1.9682367243887073]\n",
      "calc_llr: [1.5940402850823552, 0.8192674455538647]\n",
      "calc_llr: [1.628907464555101, -0.6044061593021821]\n",
      "calc_llr: [1.1757619283683902, 0.1557260165832155, -0.17371323540141548, 0.1557260165832155]\n",
      "soft:  0000101010000010\n",
      "dict: {(-1-1j): '0000', (-0.33333334-1j): '0001', (0.33333334-1j): '0011', (1-1j): '0010', (-1-0.33333334j): '0110', (-0.33333334-0.33333334j): '0111', (0.33333334-0.33333334j): '0101', (1-0.33333334j): '0100', (-1+0.33333334j): '1100', (-0.33333334+0.33333334j): '1101', (0.33333334+0.33333334j): '1111', (1+0.33333334j): '1110', (-1+1j): '1010', (-0.33333334+1j): '1011', (0.33333334+1j): '1001', (1+1j): '1000'}\n"
     ]
    }
   ],
   "source": [
    "#Sanity Checking\n",
    "print(myqam4.dict[-1-1j])\n",
    "print(myqam16.dict[-1-1j])\n",
    "print('calc_llr:', calc_llr(myqam4.dict, -1-1j, h = 1.0))\n",
    "print('calc_llr:', calc_llr(myqam4.dict, -0.5+0.75j, h = 1.0))\n",
    "print('calc_llr:', calc_llr(myqam4.dict, -0.5+0.75j, h = 1.0 + 2j))\n",
    "print('calc_llr:', calc_llr(myqam16.dict, 0.75+0.75j, h = 1.0))\n",
    "print('soft: ', soft_demodulate([-1-1j, -1+1j, 1+1j, 1-1j], myqam16.dict, h=1.0))\n",
    "print('dict:', myqam16.dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1+0j): '0000', (0.9238795+0.38268343j): '0001', (0.70710677+0.70710677j): '0011', (0.38268343+0.9238795j): '0010', (6.123234e-17+1j): '0110', (-0.38268343+0.9238795j): '0111', (-0.70710677+0.70710677j): '0101', (-0.9238795+0.38268343j): '0100', (-1+1.2246469e-16j): '1100', (-0.9238795-0.38268343j): '1101', (-0.70710677-0.70710677j): '1111', (-0.38268343-0.9238795j): '1110', (-1.8369701e-16-1j): '1010', (0.38268343-0.9238795j): '1011', (0.70710677-0.70710677j): '1001', (0.9238795-0.38268343j): '1000'}\n",
      "calc_llr: [-0.45407425350371855, -1.6281609788657778, -0.6606638425187255, -0.19617467218996398]\n",
      "calc_llr: [0.45407425350371855, 1.6281609788657778, -0.6606638425187259, -0.19617467218996398]\n",
      "calc_llr: [-2.8853900817779268]\n",
      "calc_llr: [2.8853900817779268]\n",
      "soft:  001100\n",
      "dict: {(1+0j): '0', (-1+1.2246469e-16j): '1'}\n"
     ]
    }
   ],
   "source": [
    "#Sanity Checking\n",
    "print(myPSK8.dict)\n",
    "print('calc_llr:', calc_llr(myPSK8.dict, 1, h = 1.0))\n",
    "print('calc_llr:', calc_llr(myPSK8.dict, -1, h = 1.0))\n",
    "print('calc_llr:', calc_llr(myBPSK.dict, 1, h = 1.0))\n",
    "print('calc_llr:', calc_llr(myBPSK.dict, -1, h = 1.0))\n",
    "print('soft: ', soft_demodulate([1, 1, -1, -1, 1, 1], myBPSK.dict, h=1.0))\n",
    "print('dict:', myBPSK.dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_variance: [0.010000000000000002]\n",
      "num_bits [4, 16, 64, 2, 8]\n"
     ]
    }
   ],
   "source": [
    "#Predefines for running LLR simulation\n",
    "#Generate randomized data now\n",
    "#Separate into training and test data\n",
    "%matplotlib inline\n",
    "from random import choice\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "RAND_DATA_CREATE = 64**2\n",
    "#RAND_DATA_CREATE = 8\n",
    "RAYLEIGH_FADING = np.complex64(1.0)\n",
    "#Signals have a max amplitude of 1.\n",
    "#SNR = Si - No #db\n",
    "#SNR = Si - sqrt(VAR)\n",
    "#VAR = (Si - SNR)**2. Assume Si = 1 (Most likely not the case)\n",
    "\n",
    "#SNR = Si/sqrt(VAR)\n",
    "#sqrt(VAR) = Si/SNR\n",
    "\n",
    "#SNRS = [-10, -5, -2, 1, 2, 3, 5, 10, 13, 15, 20]\n",
    "SNRS = [10]\n",
    "VARS = []\n",
    "VAR_RAYLEIGH = 1.0\n",
    "\n",
    "for snr in SNRS:\n",
    "    snr_linear = 10**(snr / 10)\n",
    "    VARS.append(np.float64(1/snr_linear)**2)\n",
    "\n",
    "print('noise_variance:' , VARS)\n",
    "    \n",
    "myModems = [myqam4, myqam16, myqam64, myBPSK, myPSK8]\n",
    "names = ['QAM', 'QAM16', 'QAM64', 'BPSK', 'PSK8']\n",
    "num_syms = [4,      16,     64,     2,        8]\n",
    "num_bits = [ k for k in (np.int_(num_syms)) ]\n",
    "history_list = []\n",
    "model_list = []\n",
    "data = ''.join(choice('01') for _ in range(RAND_DATA_CREATE))\n",
    "Adam=optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#print('qam64dict: ', myqam64.dict)\n",
    "\n",
    "print('num_bits', num_bits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAM\n",
      "X\n",
      " [[-0.83587839 -1.0149552 ]\n",
      " [ 0.92108092 -0.94568039]\n",
      " [ 1.01014705  1.1132981 ]\n",
      " ...\n",
      " [-0.6887967   0.91995281]\n",
      " [-0.91931969  0.85265487]\n",
      " [ 0.03526639  0.09406229]] \n",
      "Y\n",
      " [[-1. -1.]\n",
      " [-1.  1.]\n",
      " [ 1. -1.]\n",
      " ...\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 0.  0.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-ea7ae7775f86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mcnt_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mrun_simulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyModems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVARS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSNRS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-ea7ae7775f86>\u001b[0m in \u001b[0;36mrun_simulation\u001b[1;34m(myModems, VARS, SNRS, names, optimizer)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m#print('X_test\\n', X_test, '\\nY_test\\n', y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mmyModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mean_absolute_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mean_absolute_percentage_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmyModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m350\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0mhistory_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\applications\\python3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32md:\\applications\\python3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\applications\\python3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\applications\\python3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\applications\\python3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\applications\\python3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\applications\\python3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32md:\\applications\\python3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_simulation(myModems, VARS, SNRS, names, optimizer=Adam):\n",
    "    cnt_index = 0\n",
    "    for modem in myModems:\n",
    "        for noise_var, snr in zip(VARS, SNRS):\n",
    "            print(names[cnt_index])\n",
    "            channel = rayleigh_multipath(noise_var, VAR_RAYLEIGH, [1.0, 0.7, 0.1, 0.3])\n",
    "            myModel = generateModel(bps = modem.bit_num)\n",
    "            #Process the data such that I'm sending an even number of bits through the modulator\n",
    "            if (names[cnt_index] != 'BPSK'):\n",
    "                length = len(data)\n",
    "                num_chunks = int(np.floor_divide(length, modem.bit_num))\n",
    "                proc_data = data[:length*num_chunks]\n",
    "            else:\n",
    "                proc_data = data\n",
    "            #print('proc_data',len(proc_data),' num_chunks: ', num_chunks)\n",
    "\n",
    "            #Add AWGN and Rayleigh fading through the channel\n",
    "            TX_data = modem.TX(proc_data)\n",
    "            X_pre = (channel.awgn(TX_data))\n",
    "            #X_pre = channel.apply_cir(X_pre)\n",
    "            X = []\n",
    "            TX = []\n",
    "            for x in X_pre:\n",
    "                X.append([x.real, x.imag])\n",
    "            for x in TX_data:\n",
    "                TX.append([x.real, x.imag])\n",
    "            X = np.array(X)\n",
    "            TX = np.array(TX)\n",
    "            y = []\n",
    "            #Calculate the labels (the LLRS) of the transmitted symbols [before any channels]\n",
    "            for symbol in TX:\n",
    "                complex_symbol = symbol[0] + 1j*symbol[1]\n",
    "                #Calculate the LLRs based off of the known approximate rayleigh and gaussian variances\n",
    "                ele = calc_llr(modem.dict, complex_symbol, h=VAR_RAYLEIGH, var = noise_var)\n",
    "                ele = np.tanh(ele)\n",
    "                y.append(ele)\n",
    "\n",
    "            y = np.array(y)\n",
    "            print('X\\n', X, '\\nY\\n', y)\n",
    "            #print('Xshape\\n', X.shape, '\\nYshape\\n', y.shape)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "            #print('X_train\\n', X_train, '\\nY_train\\n', y_test)\n",
    "            #print('X_test\\n', X_test, '\\nY_test\\n', y_test)\n",
    "            myModel.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
    "            history=myModel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=350, verbose=0)\n",
    "            history_list.append(history)\n",
    "\n",
    "            model_list.append(myModel)\n",
    "            fig = plt.figure()\n",
    "            ax = plt.gca()\n",
    "\n",
    "            ax.set_ylim(0, 1)\n",
    "\n",
    "            plt.plot(history.history['mean_squared_error'])\n",
    "            plt.plot(history.history['val_mean_squared_error'])\n",
    "            plt.title('Model Mean-Squared-Error %s SNR: %s' % (names[cnt_index], snr))\n",
    "            plt.ylabel('MSE')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(['MSE', 'Validation MSE'], loc = 'upper left')\n",
    "            plt.show()\n",
    "\n",
    "        cnt_index += 1\n",
    "\n",
    "run_simulation(myModems, VARS, SNRS, names, optimizer=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BPSK_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e6ac9dc81388>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#BPSK_model = model_list[3]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BPSK negative:'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mBPSK_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m9.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BPSK positive:'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mBPSK_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BPSK_model' is not defined"
     ]
    }
   ],
   "source": [
    "#Debug BPSK\n",
    "\n",
    "#BPSK_model = model_list[3]\n",
    "print('BPSK negative:' , BPSK_model.predict(np.array([[-9.0, 0.0]])))\n",
    "print('BPSK positive:' , BPSK_model.predict(np.array([[1.0, 0.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "use_mse = True\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
    "\n",
    "    # Arguments\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = (n - 1) * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "def run_vae_sim(X_train, X_test, y_train, y_test, original_dim, intermediate_dim, batch_size, latent_dim, epochs, use_mse=True):\n",
    "    #image_size = x_train.shape[1]\n",
    "    #original_dim = image_size * image_size\n",
    "    #x_train = np.reshape(x_train, [-1, original_dim])\n",
    "    #x_test = np.reshape(x_test, [-1, original_dim])\n",
    "    #x_train = x_train.astype('float32') / 255\n",
    "    #x_test = x_test.astype('float32') / 255\n",
    "\n",
    "    # network parameters\n",
    "    input_shape = (original_dim, )\n",
    "\n",
    "    print('input_shape', input_shape)\n",
    "    \n",
    "    # VAE model = encoder + decoder\n",
    "    # build encoder model\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    print('input_shape', input_shape)\n",
    "    x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    # instantiate encoder model\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    encoder.summary()\n",
    "    plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "    # build decoder model\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "    outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "    # instantiate decoder model\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "    plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "    # instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
    "    if use_mse:\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "    else:\n",
    "        reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                                  outputs)\n",
    "\n",
    "    models = (encoder,decoder)\n",
    "    data = (X_test, y_test)\n",
    "    reconstruction_loss *= original_dim\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer=Adam, metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
    "    vae.summary()\n",
    "    plot_model(vae,\n",
    "               to_file='vae_mlp.png',\n",
    "               show_shapes=True)\n",
    "\n",
    "    if False:\n",
    "        vae.load_weights(args.weights)\n",
    "    else:\n",
    "        # train the autoencoder\n",
    "        history = vae.fit(X_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(X_test, None))\n",
    "        #vae.save_weights('vae_mlp_mnist.h5')\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    plt.plot(history.history['mean_squared_error'])\n",
    "    plt.plot(history.history['val_mean_squared_error'])\n",
    "    plt.title('Model Mean-Squared-Error %s SNR: %s' % (names[cnt_index], snr))\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['MSE', 'Validation MSE'], loc = 'upper left')\n",
    "    plt.show()\n",
    "        \n",
    "    '''\n",
    "    plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=batch_size,\n",
    "                 model_name=\"vae_mlp\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process data similar to forward-connected example.\n",
    "\n",
    "def run_simulation_vae(myModems, VARS, SNRS, names, optimizer='Adam'):\n",
    "    return_list = []\n",
    "    cnt_index = 0\n",
    "    for modem in myModems:\n",
    "        for noise_var, snr in zip(VARS, SNRS):\n",
    "            print(names[cnt_index])\n",
    "            channel = rayleigh_multipath(noise_var, VAR_RAYLEIGH, [1.0, 0.7, 0.1, 0.3])\n",
    "            myModel = generateModel(bps = modem.bit_num)\n",
    "            #Process the data such that I'm sending an even number of bits through the modulator\n",
    "            if (names[cnt_index] != 'BPSK'):\n",
    "                length = len(data)\n",
    "                num_chunks = int(np.floor_divide(length, modem.bit_num))\n",
    "                proc_data = data[:length*num_chunks]\n",
    "            else:\n",
    "                proc_data = data\n",
    "            #print('proc_data',len(proc_data),' num_chunks: ', num_chunks)\n",
    "\n",
    "            #Add AWGN and Rayleigh fading through the channel\n",
    "            TX_data = modem.TX(proc_data)\n",
    "            X_pre = (channel.awgn(TX_data))\n",
    "            #X_pre = channel.apply_cir(X_pre)\n",
    "            X = []\n",
    "            TX = []\n",
    "            for x in X_pre:\n",
    "                X.append([x.real, x.imag])\n",
    "            for x in TX_data:\n",
    "                TX.append([x.real, x.imag])\n",
    "            X = np.array(X)\n",
    "            TX = np.array(TX)\n",
    "            y = []\n",
    "            #Calculate the labels (the LLRS) of the transmitted symbols [before any channels]\n",
    "            for symbol in TX:\n",
    "                complex_symbol = symbol[0] + 1j*symbol[1]\n",
    "                #Calculate the LLRs based off of the known approximate rayleigh and gaussian variances\n",
    "                ele = calc_llr(modem.dict, complex_symbol, h=VAR_RAYLEIGH, var = noise_var)\n",
    "                ele = np.tanh(ele)\n",
    "                y.append(ele)\n",
    "\n",
    "            y = np.array(y)\n",
    "            #print('X\\n', X, '\\nY\\n', y)\n",
    "            print('Xshape\\n', X.shape, '\\nY.shape\\n', y.shape)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "            print('Xtrainshape\\n', X_train.shape, '\\nYtrain.shape\\n', y_train.shape)\n",
    "            print('Xtestshape\\n', X_test.shape, '\\nYtest.shape\\n', y_test.shape)\n",
    "            #Train VAE\n",
    "            #X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "            #X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
    "            #print('input_dim: ', input_dim)\n",
    "            #vae, encoder, generator, loss = generate_model_vae(input_dim, input_dim, input_dim, batch_size, use_mse = True, optimizer = 'RMSProp')\n",
    "            batch_size = 100\n",
    "            epochs = 100\n",
    "            run_vae_sim(X_train, X_test, y_train, y_test, 2, 2, batch_size, y_train.shape[1], epochs, use_mse=True)\n",
    "            cnt_index += 1\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAM\n",
      "Xshape\n",
      " (2048, 2) \n",
      "Y.shape\n",
      " (2048, 2)\n",
      "Xtrainshape\n",
      " (1638, 2) \n",
      "Ytrain.shape\n",
      " (1638, 2)\n",
      "Xtestshape\n",
      " (410, 2) \n",
      "Ytest.shape\n",
      " (410, 2)\n",
      "input_shape (2,)\n",
      "input_shape (2,)\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_192 (Dense)               (None, 2)            6           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            6           dense_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            6           dense_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 18\n",
      "Trainable params: 18\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 18        \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1638 samples, validate on 410 samples\n",
      "Epoch 1/100\n",
      "1638/1638 [==============================] - 0s 108us/step - loss: 3.0464 - val_loss: 2.8885\n",
      "Epoch 2/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.7382 - val_loss: 2.6925\n",
      "Epoch 3/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.5806 - val_loss: 2.5811\n",
      "Epoch 4/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.4956 - val_loss: 2.4832\n",
      "Epoch 5/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.4317 - val_loss: 2.4517\n",
      "Epoch 6/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.4074 - val_loss: 2.3801\n",
      "Epoch 7/100\n",
      "1638/1638 [==============================] - 0s 19us/step - loss: 2.3411 - val_loss: 2.3497\n",
      "Epoch 8/100\n",
      "1638/1638 [==============================] - 0s 19us/step - loss: 2.3283 - val_loss: 2.3443\n",
      "Epoch 9/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.3037 - val_loss: 2.3435\n",
      "Epoch 10/100\n",
      "1638/1638 [==============================] - 0s 19us/step - loss: 2.2728 - val_loss: 2.2864\n",
      "Epoch 11/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.2669 - val_loss: 2.2654\n",
      "Epoch 12/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.2555 - val_loss: 2.2537\n",
      "Epoch 13/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.2536 - val_loss: 2.2339\n",
      "Epoch 14/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.2409 - val_loss: 2.2595\n",
      "Epoch 15/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.2341 - val_loss: 2.2506\n",
      "Epoch 16/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.2003 - val_loss: 2.2680\n",
      "Epoch 17/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.2106 - val_loss: 2.2391\n",
      "Epoch 18/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.1964 - val_loss: 2.2172\n",
      "Epoch 19/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.2124 - val_loss: 2.2049\n",
      "Epoch 20/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1870 - val_loss: 2.2185\n",
      "Epoch 21/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.1746 - val_loss: 2.2099\n",
      "Epoch 22/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.1699 - val_loss: 2.2377\n",
      "Epoch 23/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.1678 - val_loss: 2.1992\n",
      "Epoch 24/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.1653 - val_loss: 2.1783\n",
      "Epoch 25/100\n",
      "1638/1638 [==============================] - 0s 14us/step - loss: 2.1668 - val_loss: 2.1588\n",
      "Epoch 26/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1489 - val_loss: 2.1758\n",
      "Epoch 27/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1541 - val_loss: 2.1652\n",
      "Epoch 28/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.1637 - val_loss: 2.1549\n",
      "Epoch 29/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1349 - val_loss: 2.2136\n",
      "Epoch 30/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.1503 - val_loss: 2.1744\n",
      "Epoch 31/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1361 - val_loss: 2.1652\n",
      "Epoch 32/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1322 - val_loss: 2.1699\n",
      "Epoch 33/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1168 - val_loss: 2.1534\n",
      "Epoch 34/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1147 - val_loss: 2.1643\n",
      "Epoch 35/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1157 - val_loss: 2.1405\n",
      "Epoch 36/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.1268 - val_loss: 2.1413\n",
      "Epoch 37/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1309 - val_loss: 2.1328\n",
      "Epoch 38/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.1190 - val_loss: 2.1446\n",
      "Epoch 39/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1113 - val_loss: 2.1261\n",
      "Epoch 40/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.1107 - val_loss: 2.1651\n",
      "Epoch 41/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.0950 - val_loss: 2.1495\n",
      "Epoch 42/100\n",
      "1638/1638 [==============================] - 0s 14us/step - loss: 2.0977 - val_loss: 2.1291\n",
      "Epoch 43/100\n",
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.0937 - val_loss: 2.1115\n",
      "Epoch 44/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.0866 - val_loss: 2.1037\n",
      "Epoch 45/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.1094 - val_loss: 2.1133\n",
      "Epoch 46/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.1077 - val_loss: 2.1361\n",
      "Epoch 47/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.0876 - val_loss: 2.1245\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1638/1638 [==============================] - 0s 17us/step - loss: 2.0972 - val_loss: 2.1053\n",
      "Epoch 49/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1079 - val_loss: 2.1150\n",
      "Epoch 50/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.0898 - val_loss: 2.1055\n",
      "Epoch 51/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1110 - val_loss: 2.1101\n",
      "Epoch 52/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0770 - val_loss: 2.1021\n",
      "Epoch 53/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0856 - val_loss: 2.0954\n",
      "Epoch 54/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0997 - val_loss: 2.1019\n",
      "Epoch 55/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.0926 - val_loss: 2.1183\n",
      "Epoch 56/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0890 - val_loss: 2.1259\n",
      "Epoch 57/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.1026 - val_loss: 2.1123\n",
      "Epoch 58/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0825 - val_loss: 2.1119\n",
      "Epoch 59/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0930 - val_loss: 2.0989\n",
      "Epoch 60/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0796 - val_loss: 2.0926\n",
      "Epoch 61/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0814 - val_loss: 2.1197\n",
      "Epoch 62/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0988 - val_loss: 2.1359\n",
      "Epoch 63/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0663 - val_loss: 2.0938\n",
      "Epoch 64/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0871 - val_loss: 2.1089\n",
      "Epoch 65/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0794 - val_loss: 2.1052\n",
      "Epoch 66/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0844 - val_loss: 2.0998\n",
      "Epoch 67/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0849 - val_loss: 2.0982\n",
      "Epoch 68/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.0795 - val_loss: 2.0858\n",
      "Epoch 69/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.0841 - val_loss: 2.0756\n",
      "Epoch 70/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0556 - val_loss: 2.0968\n",
      "Epoch 71/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0809 - val_loss: 2.0681\n",
      "Epoch 72/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0619 - val_loss: 2.0707\n",
      "Epoch 73/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.0572 - val_loss: 2.0934\n",
      "Epoch 74/100\n",
      "1638/1638 [==============================] - 0s 14us/step - loss: 2.0849 - val_loss: 2.0941\n",
      "Epoch 75/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0620 - val_loss: 2.0853\n",
      "Epoch 76/100\n",
      "1638/1638 [==============================] - 0s 14us/step - loss: 2.0836 - val_loss: 2.0819\n",
      "Epoch 77/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0730 - val_loss: 2.0610\n",
      "Epoch 78/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0817 - val_loss: 2.0619\n",
      "Epoch 79/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0749 - val_loss: 2.0848\n",
      "Epoch 80/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.0772 - val_loss: 2.0903\n",
      "Epoch 81/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0555 - val_loss: 2.0854\n",
      "Epoch 82/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0716 - val_loss: 2.1075\n",
      "Epoch 83/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0681 - val_loss: 2.0653\n",
      "Epoch 84/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0642 - val_loss: 2.0642\n",
      "Epoch 85/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.0647 - val_loss: 2.0872\n",
      "Epoch 86/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.0575 - val_loss: 2.0961\n",
      "Epoch 87/100\n",
      "1638/1638 [==============================] - 0s 18us/step - loss: 2.0744 - val_loss: 2.0711\n",
      "Epoch 88/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0570 - val_loss: 2.0835\n",
      "Epoch 89/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0599 - val_loss: 2.0694\n",
      "Epoch 90/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0576 - val_loss: 2.0535\n",
      "Epoch 91/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0620 - val_loss: 2.0674\n",
      "Epoch 92/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.0594 - val_loss: 2.0807\n",
      "Epoch 93/100\n",
      "1638/1638 [==============================] - 0s 13us/step - loss: 2.0705 - val_loss: 2.0773\n",
      "Epoch 94/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0622 - val_loss: 2.0685\n",
      "Epoch 95/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0546 - val_loss: 2.0751\n",
      "Epoch 96/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0490 - val_loss: 2.0616\n",
      "Epoch 97/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0526 - val_loss: 2.0940\n",
      "Epoch 98/100\n",
      "1638/1638 [==============================] - 0s 15us/step - loss: 2.0602 - val_loss: 2.0765\n",
      "Epoch 99/100\n",
      "1638/1638 [==============================] - 0s 14us/step - loss: 2.0601 - val_loss: 2.0724\n",
      "Epoch 100/100\n",
      "1638/1638 [==============================] - 0s 16us/step - loss: 2.0619 - val_loss: 2.0592\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mean_squared_error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-3ddd33f7f4a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#X_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#X_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrun_simulation_vae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyModems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVARS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSNRS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-24b6ce9650f6>\u001b[0m in \u001b[0;36mrun_simulation_vae\u001b[1;34m(myModems, VARS, SNRS, names, optimizer)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mrun_vae_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_mse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0mcnt_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-fd0cce11db6b>\u001b[0m in \u001b[0;36mrun_vae_sim\u001b[1;34m(X_train, X_test, y_train, y_test, original_dim, intermediate_dim, batch_size, latent_dim, epochs, use_mse)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_mean_squared_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Mean-Squared-Error %s SNR: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnt_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mean_squared_error'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#X_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "#X_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "run_simulation_vae(myModems, VARS, SNRS, names, optimizer=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
